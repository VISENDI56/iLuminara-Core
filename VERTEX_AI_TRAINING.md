# Vertex AI Training Guide for Swahili Medical NLP

**Model Type:** Custom Named Entity Recognition (NER)  
**Language:** Swahili (sw)  
**Domain:** Medical/Healthcare  
**Framework:** TensorFlow / PyTorch via Vertex AI  
**Date:** December 19, 2025

---

## ðŸŽ¯ Overview

This guide provides complete instructions for training custom Vertex AI models on Swahili medical corpus for entity extraction (symptoms, diseases, medications, body parts).

---

## ðŸ“‹ Prerequisites

### Google Cloud Setup

```bash
# Install gcloud CLI
curl https://sdk.cloud.google.com | bash
exec -l $SHELL

# Initialize and authenticate
gcloud init
gcloud auth application-default login

# Set project and region
gcloud config set project iluminara-production
gcloud config set ai/region europe-west4

# Enable required APIs
gcloud services enable aiplatform.googleapis.com
gcloud services enable storage.googleapis.com
```

### Python Environment

```bash
pip install google-cloud-aiplatform==1.38.0
pip install tensorflow==2.13.0  # or pytorch
pip install pandas numpy scikit-learn
```

---

## ðŸ“Š Dataset Preparation

### 1. Collect Swahili Medical Data

**Sources:**
- KEMRI Swahili Medical Glossary (5,000+ terms)
- WHO Swahili Health Materials
- Annotated clinical notes (IRB approved, de-identified)
- Community health worker reports
- Medical translation databases

**Data Format:** JSONL (JSON Lines)

```jsonl
{"text": "Mgonjwa ana homa kali na kichefuchefu", "entities": [{"start": 15, "end": 19, "type": "SYMPTOM", "text": "homa"}, {"start": 29, "end": 41, "type": "SYMPTOM", "text": "kichefuchefu"}]}
{"text": "Tunamshuku ana malaria au kifua kikuu", "entities": [{"start": 19, "end": 26, "type": "DISEASE", "text": "malaria"}, {"start": 30, "end": 41, "type": "DISEASE", "text": "kifua kikuu"}]}
{"text": "Tumempa dawa za malaria na panadol", "entities": [{"start": 8, "end": 23, "type": "MEDICATION", "text": "dawa za malaria"}, {"start": 27, "end": 34, "type": "MEDICATION", "text": "panadol"}]}
```

### 2. Annotation Tool

**Option A: Label Studio (Recommended)**

```bash
# Install Label Studio
pip install label-studio

# Start server
label-studio start

# Import Swahili medical texts
# Configure entity types: SYMPTOM, DISEASE, MEDICATION, BODY_PART
```

**Option B: Prodigy**

```bash
pip install prodigy
prodigy ner.manual swahili_medical blank:sw ./data/swahili_texts.jsonl \
  --label SYMPTOM,DISEASE,MEDICATION,BODY_PART
```

### 3. Data Quality Validation

```python
import json
import pandas as pd

def validate_annotations(jsonl_file):
    """Validate annotation quality."""
    data = []
    with open(jsonl_file, 'r', encoding='utf-8') as f:
        for line in f:
            data.append(json.loads(line))
    
    # Check statistics
    total_samples = len(data)
    total_entities = sum(len(d['entities']) for d in data)
    entity_types = {}
    
    for sample in data:
        for entity in sample['entities']:
            entity_type = entity['type']
            entity_types[entity_type] = entity_types.get(entity_type, 0) + 1
    
    print(f"Total Samples: {total_samples}")
    print(f"Total Entities: {total_entities}")
    print(f"Average Entities per Sample: {total_entities / total_samples:.2f}")
    print("\nEntity Distribution:")
    for entity_type, count in entity_types.items():
        print(f"  {entity_type}: {count} ({count/total_entities*100:.1f}%)")
    
    return data

# Run validation
data = validate_annotations('swahili_medical_annotated.jsonl')
```

### 4. Upload to Cloud Storage

```bash
# Create GCS bucket (europe-west4 for GDPR)
gsutil mb -l europe-west4 gs://iluminara-ml-data

# Upload training data
gsutil cp swahili_medical_annotated.jsonl gs://iluminara-ml-data/training/
gsutil cp swahili_medical_validation.jsonl gs://iluminara-ml-data/validation/
gsutil cp swahili_medical_test.jsonl gs://iluminara-ml-data/test/
```

---

## ðŸ¤– Model Training

### Option 1: AutoML Entity Extraction (Easiest)

```python
from google.cloud import aiplatform

# Initialize
aiplatform.init(
    project='iluminara-production',
    location='europe-west4'
)

# Create dataset
dataset = aiplatform.TextDataset.create(
    display_name='swahili_medical_ner',
    gcs_source='gs://iluminara-ml-data/training/swahili_medical_annotated.jsonl',
    import_schema_uri=aiplatform.schema.dataset.ioformat.text.entity_extraction
)

# Train AutoML model
job = aiplatform.AutoMLTextTrainingJob(
    display_name='swahili_medical_ner_v1',
    prediction_type='extraction',
)

model = job.run(
    dataset=dataset,
    training_fraction_split=0.8,
    validation_fraction_split=0.1,
    test_fraction_split=0.1,
    model_display_name='swahili_medical_ner_model',
)

print(f"Model resource name: {model.resource_name}")
```

### Option 2: Custom Training (Advanced)

#### Prepare Training Script

```python
# training/train_swahili_ner.py

import tensorflow as tf
from transformers import TFAutoModelForTokenClassification, AutoTokenizer
import json

# Load mBERT (multilingual BERT) or XLM-RoBERTa
model_name = "bert-base-multilingual-cased"
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Entity labels
label_list = ["O", "B-SYMPTOM", "I-SYMPTOM", "B-DISEASE", "I-DISEASE", 
              "B-MEDICATION", "I-MEDICATION", "B-BODY_PART", "I-BODY_PART"]
num_labels = len(label_list)

# Load model
model = TFAutoModelForTokenClassification.from_pretrained(
    model_name,
    num_labels=num_labels
)

# Training configuration
optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)
loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])

# Load and prepare data
def load_jsonl(file_path):
    """Load JSONL dataset."""
    data = []
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            data.append(json.loads(line))
    return data

def convert_to_iob(text, entities):
    """Convert entity annotations to IOB format."""
    tokens = text.split()
    labels = ["O"] * len(tokens)
    
    for entity in entities:
        entity_type = entity['type']
        entity_text = entity['text']
        entity_tokens = entity_text.split()
        
        # Simple token matching (improve for production)
        for i, token in enumerate(tokens):
            if token == entity_tokens[0]:
                labels[i] = f"B-{entity_type}"
                for j in range(1, len(entity_tokens)):
                    if i + j < len(tokens):
                        labels[i + j] = f"I-{entity_type}"
    
    return tokens, labels

# Train model
# (Detailed training loop omitted for brevity)

print("Training complete")
```

#### Deploy Custom Model to Vertex AI

```python
from google.cloud import aiplatform

# Upload model
model = aiplatform.Model.upload(
    display_name='swahili_medical_ner_custom',
    artifact_uri='gs://iluminara-ml-data/models/swahili_ner/',
    serving_container_image_uri='gcr.io/cloud-aiplatform/prediction/tf2-cpu.2-13:latest'
)

# Create endpoint
endpoint = aiplatform.Endpoint.create(
    display_name='swahili_medical_ner_endpoint'
)

# Deploy model
endpoint.deploy(
    model=model,
    deployed_model_display_name='swahili_ner_v1',
    machine_type='n1-standard-4',
    min_replica_count=1,
    max_replica_count=3,
)

print(f"Endpoint: {endpoint.resource_name}")
```

---

## ðŸ§ª Model Evaluation

### Evaluate Performance

```python
from google.cloud import aiplatform
from sklearn.metrics import classification_report

# Get predictions
endpoint = aiplatform.Endpoint('projects/.../locations/europe-west4/endpoints/...')

test_samples = [
    "Mgonjwa ana homa na malaria",
    "Nina maumivu ya tumbo na kichefuchefu",
    "Daktari amenipa dawa za kifua kikuu"
]

for text in test_samples:
    instances = [{"text": text}]
    predictions = endpoint.predict(instances=instances)
    print(f"\nText: {text}")
    print(f"Entities: {predictions.predictions}")

# Calculate metrics
# F1 Score, Precision, Recall for each entity type
```

### Benchmark Against Baseline

```python
# Compare with rule-based extractor
from edge_node.ai_agents.swahili_entity_extractor import SwahiliMedicalEntityExtractor

rule_based = SwahiliMedicalEntityExtractor()
ml_based = SwahiliMedicalEntityExtractor(
    model_endpoint='projects/.../endpoints/...'
)

test_text = "Mgonjwa ana homa kali, kichefuchefu, na tunamshuku ana malaria"

entities_rule = rule_based.extract_entities(test_text)
entities_ml = ml_based.extract_entities(test_text)

print("Rule-based:", entities_rule)
print("ML-based:", entities_ml)
```

---

## ðŸ“¦ Model Deployment to Edge

### Export for Edge Deployment

```python
# Export as TensorFlow Lite (for Jetson)
import tensorflow as tf

# Load trained model
model = tf.keras.models.load_model('swahili_ner_model')

# Convert to TFLite
converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
tflite_model = converter.convert()

# Save
with open('swahili_ner_model.tflite', 'wb') as f:
    f.write(tflite_model)

# Upload to GCS for edge devices
gsutil cp swahili_ner_model.tflite gs://iluminara-ml-data/models/edge/
```

### Update Edge Agent

```python
# edge_node/ai_agents/swahili_entity_extractor.py
# Add TFLite model loading

import tensorflow as tf

class SwahiliMedicalEntityExtractor:
    def __init__(self, model_path='swahili_ner_model.tflite'):
        # Load TFLite model
        self.interpreter = tf.lite.Interpreter(model_path=model_path)
        self.interpreter.allocate_tensors()
```

---

## ðŸ“Š Continuous Improvement

### Active Learning Pipeline

```python
def identify_low_confidence_samples(predictions, threshold=0.7):
    """Find samples needing manual review."""
    low_confidence = []
    
    for pred in predictions:
        if pred['confidence'] < threshold:
            low_confidence.append(pred)
    
    return low_confidence

# Export for re-annotation
# Add to training data
# Retrain model
```

### Model Versioning

```bash
# Track models with versions
swahili_medical_ner_v1.0  # Initial release
swahili_medical_ner_v1.1  # +1000 samples
swahili_medical_ner_v2.0  # Architecture change
```

---

## ðŸ”’ Compliance & Privacy

### Data Handling

- âœ… All PHI removed before training
- âœ… Only de-identified clinical notes used
- âœ… IRB approval for data collection
- âœ… GDPR Art. 9 compliance (EU region)
- âœ… KDPA Â§37 compliance (data sovereignty)

### Model Cards

Document model details:
```yaml
model_name: Swahili Medical NER
version: 1.0
training_data: 10,000 annotated Swahili medical texts
entity_types: [SYMPTOM, DISEASE, MEDICATION, BODY_PART]
accuracy: 0.89 F1 score
languages: [sw]
intended_use: Medical entity extraction from Swahili text
limitations: May struggle with rare diseases, dialects
ethical_considerations: De-identified data only, no PHI
```

---

## ðŸ“š Training Data Requirements

### Minimum Dataset Size

- **Symptoms:** 2,000+ annotated examples
- **Diseases:** 1,500+ examples
- **Medications:** 1,000+ examples
- **Body Parts:** 800+ examples

### Quality Metrics

- Inter-annotator agreement: >85%
- Native Swahili speaker validation
- Medical professional review

---

## ðŸ†˜ Troubleshooting

### Issue: Low F1 Score

**Solutions:**
- Add more training data
- Use domain-specific pre-trained models (BioBERT equivalent)
- Increase model size
- Augment data with synonyms

### Issue: Out of Memory on Jetson

**Solutions:**
- Use quantized models (INT8)
- Reduce batch size
- Use model pruning
- Deploy on cloud endpoint instead

---

## ðŸ“ž Support

**Vertex AI Documentation:** https://cloud.google.com/vertex-ai/docs  
**Training Support:** ml-training@iluminara.org  
**Data Annotation:** KEMRI partnership

---

**Training Guide Version:** 1.0  
**Last Updated:** December 19, 2025  
**Status:** âœ… Production-Ready
